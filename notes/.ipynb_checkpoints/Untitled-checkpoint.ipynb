{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Exploratory Data Analysis Notes](#exploratory-data-analysis-notes)\n",
    "  - [1. Read, Clean and Validate:](#1-read-clean-and-validate)\n",
    "    - [1.1 Reading](#11-reading)\n",
    "    - [1.2 Cleaning](#12-cleaning)\n",
    "    - [1.3 Validating](#13-validating)\n",
    "  - [2. Distributions:](#2-distributions)\n",
    "    - [2.1 Histograms](#21-histograms)\n",
    "    - [2.2 PMFs (Probability Mass Functions)](#22-pmfs-probability-mass-functions)\n",
    "    - [2.3 CDFs (Cumulative Distribution Functions)](#23-cdfs-cumulative-distribution-functions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis Notes\n",
    "\n",
    "## 1. Read, Clean and Validate:\n",
    "\n",
    "First, we need to read the data from the source. It could be a database, a csv file or other formats. After that, it's highly recommended to clean your data, *i.e.*, remove or treat any missing, invalid, incomplete or corrupted data.  \n",
    "\n",
    "### 1.1 Reading  \n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "data = pd.read_csv('../data/billboard.csv')\n",
    "```\n",
    "\n",
    "**PS: Always remember to constantly check the data documentation before treating it.** \n",
    "\n",
    "### 1.2 Cleaning\n",
    "\n",
    "* #### Replacing\n",
    "   We can use this technique to replace outliers, missing values, corrupted data and many more. The value to replace with could be any value you want, but there are some methods like mean, median, or, for example, replacing the outliers with the max/min non-outlier values. It's up to you infer the best approach here.  \n",
    "\n",
    "```python\n",
    "#Replace all the <value_to_be_replaced> values with <value_to_replace_with>\n",
    "data[<column>].replace(to_replace=<value_to_be_replaced>,\n",
    "                        value=<value_to_replace_with>,\n",
    "                        inplace=True)\n",
    "```  \n",
    "\n",
    "* #### Filling\n",
    "   Filling is most used to \n",
    "```python\n",
    "#Fill all NA/null values of <column> with <value>\n",
    "data[<column>].fillna(<value>)\n",
    "```  \n",
    "\n",
    "* #### Trimming\n",
    "   \n",
    "`Row` dropping method\n",
    "```python\n",
    "#Drop all the rows with the NA/null values in the columns specified\n",
    "data.dropna(subset=<columns_list>, inplace=True, axis='index')\n",
    "```  \n",
    "\n",
    "`Column` dropping method\n",
    "```python\n",
    "#Drop all the rows with the NA/null values in the columns specified\n",
    "data.dropna(subset=<columns_list>, inplace=True, axis='columns')\n",
    "```  \n",
    "\n",
    "### 1.3 Validating\n",
    "\n",
    "## 2. Distributions:\n",
    "\n",
    "To analyze a variable distribution, we use some of the following techniques:  \n",
    "\n",
    "### 2.1 Histograms\n",
    "   \n",
    "Histograms are the simplest way to visualize a variable distribution. They count how many times a range of values occurs in the given data.  \n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "'''Remember that this method doesn't handle missing data, so it should be\n",
    "treated before using it. If not, just call .dropna() method after the series\n",
    "name.\n",
    "'''\n",
    "plt.hist(<data_series>)\n",
    "```  \n",
    "\n",
    "\n",
    "### 2.2 PMFs (Probability Mass Functions)\n",
    "    \n",
    "PMFs works similarly to Histograms, but it tells the probability, drawing a random element of the data, that this element is X, *i.e.*, the desired value.  \n",
    "To work with PMFs we're going to use the [empiricaldist] python library, created by Allen Downey for DataCamp's EDA in python course.  \n",
    "\n",
    "```python\n",
    "from empiricaldist import Pmf\n",
    "\n",
    "pmf = Pmf(<data_series>, normalize=True) #this returns a Pmf object\n",
    "\n",
    "#we can also plot the PMF as shown below:\n",
    "pmf.bar()\n",
    "plt.show()\n",
    "```  \n",
    "\n",
    "\n",
    "### 2.3 CDFs (Cumulative Distribution Functions)\n",
    "\n",
    "The CDF is the probability of getting a value <= X, drawing a random element of the data. In other words, the CDF is the cumulative sum of PMF.  \n",
    "Here, we're also going to use the [empiricaldist] python library, mentioned above. The result of ```Cdf(X)``` is the amount, between 0 and 1, of the values that are **less** or **equal** than X.  \n",
    "\n",
    "```python\n",
    "from empiricaldist import Cdf\n",
    "\n",
    "cdf = Cdf(<data_series>) #this returns a Cdf object\n",
    "\n",
    "#we can also plot the CDF as shown below:\n",
    "cdf.plot()\n",
    "plt.show()\n",
    "```  \n",
    "We can also use the inverted version of this function to get the percentiles, *i.e.*, given a `p` percentile, `cdf.inverse(p)` returns the value corresponding to the p-th percentile.   \n",
    "\n",
    "For example:  \n",
    "\n",
    "```python\n",
    "from empiricaldist import Cdf\n",
    "\n",
    "cdf = Cdf(<data_series>) #this returns a Cdf object\n",
    "\n",
    "p = 0.25\n",
    "q = cdf.inverse(p) #this returns the 25th percentile\n",
    "```  \n",
    "In the code above, `q` corresponds to the **25th percentile** of `<data_series>`.\n",
    "\n",
    "\n",
    "[empiricaldist]: https://pypi.org/project/empiricaldist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
